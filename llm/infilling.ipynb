{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfppl  import Model, CachedCausalLM, Token, LMContext, smc_standard, TokenCategorical\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = CachedCausalLM.from_pretrained(\"lmsys/Vicuna-7b-v1.5\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfillingModel(Model):\n",
    "    def __init__(self, LLM, prompt, max_tokens):\n",
    "        super().__init__()\n",
    "        self.parts = prompt.split(\"[BLANK]\")\n",
    "        self.lm = LMContext(LLM, self.parts[0])\n",
    "        self.max_tokens = max_tokens\n",
    "        self.current_part_index = 1\n",
    "        self.generated_text = self.parts[0]  # Initialize with the first part of the prompt\n",
    "\n",
    "    async def step(self):\n",
    "        if self.current_part_index >= len(self.parts):\n",
    "            self.finish()\n",
    "            print(f\"Generated: {self.generated_text}\")\n",
    "            return\n",
    "\n",
    "        n = self.sample_geom(0.5) + 1  # Number of tokens to generate\n",
    "        for _ in range(n):\n",
    "            token = await self.sample(self.lm.next_token(), proposal=self.lm.next_token())\n",
    "            self.generated_text += str(token)\n",
    "            # Update LMContext with the newly generated token\n",
    "            self.lm = LMContext(LLM, self.generated_text)\n",
    "            await self.observe(self.lm.next_token(), token.token_id)\n",
    "\n",
    "        # Add the next part of the prompt after the blank and update LMContext\n",
    "        if self.current_part_index < len(self.parts):\n",
    "            self.generated_text += self.parts[self.current_part_index]\n",
    "            self.lm = LMContext(self.lm.lm, self.generated_text)\n",
    "            self.current_part_index += 1\n",
    "\n",
    "    def sample_geom(self, p):\n",
    "        return np.random.geometric(p) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Yesterday I went to the country [BLANK] and bought a [BLANK]\"\"\"\n",
    "\n",
    "LLM.cache_kv(LLM.tokenizer.encode(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_infilling():\n",
    "    infilling_model = InfillingModel(LLM=LLM, prompt=prompt, max_tokens=50)\n",
    "    particles = await smc_standard(infilling_model, 20)\n",
    "    for p in particles:\n",
    "        print(p.generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_infilling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS = {i : set(j for (j,v) in enumerate(LLM.vocab)\n",
    "                 if j != LLM.tokenizer.eos_token_id and '\\n' not in v and\n",
    "                 any(c.isalpha() or c in punctuation for c in v) and\n",
    "                 len(v.strip()) <= 5 and (not v[0].isalpha() or i+len(v) <= 5))\n",
    "             for i in range(6)}\n",
    "\n",
    "class ConstraintModel(Model):\n",
    "    def __init__(self, prompt, max_tokens):\n",
    "        super().__init__()\n",
    "        self.lm         = LMContext(LLM, prompt)\n",
    "        self.q          = LMContext(LLM, prompt)\n",
    "        self.prompt_len = len(str(self.lm.s))\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "\n",
    "    async def step(self):\n",
    "        # Which tokens are allowed?\n",
    "        mask = self.active_constraint_mask()\n",
    "\n",
    "        # Generate proposed token.\n",
    "        token = await self.sample(self.lm.next_token(),\n",
    "                                  proposal = await self.proposal(mask))\n",
    "\n",
    "        # Condition on constraint â€” a no-op since proposal already guarantees the constraint\n",
    "        self.condition(token.token_id in mask)\n",
    "\n",
    "        # Reduce number of max tokens remaining\n",
    "        self.max_tokens -= 1\n",
    "\n",
    "        print(str(self.lm.s)[self.prompt_len:])\n",
    "\n",
    "        # Check if done\n",
    "        if token == LLM.tokenizer.eos_token_id or self.max_tokens == 0:\n",
    "            self.finish()\n",
    "\n",
    "    def active_constraint_mask(self):\n",
    "        string_so_far = str(self.lm.s)\n",
    "        words = string_so_far.split()\n",
    "        last_word = words[-1] if len(words) > 0 else \"\"\n",
    "        return MASKS[min(5, len(last_word))]\n",
    "\n",
    "    async def proposal(self, mask):\n",
    "        string_so_far = str(self.lm.s)\n",
    "\n",
    "        # Force the proposal StatefulLM to adhere to this mask\n",
    "        await self.intervene(self.q.mask_dist(mask), True)\n",
    "\n",
    "        # Return the proposal's modified next-token distribution\n",
    "        return self.q.next_token()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
